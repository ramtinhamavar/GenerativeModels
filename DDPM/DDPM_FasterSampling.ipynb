{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ac963",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 0.0001 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_TIMESTEPS = 1000\n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e59486",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x*2 - 1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_of_random_numbers(samples, num_of_samples):\n",
    "    selected_samples = list()\n",
    "    for _ in range(num_of_samples):\n",
    "        selected_samples.append(random.choice(samples))\n",
    "    \n",
    "    return np.sum(selected_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler():\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end, device):\n",
    "        \n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1. - self.alpha_cum_prod)\n",
    "    \n",
    "    def add_noise(self, original, noise, t):\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        \n",
    "        sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha_cum_prod*original + sqrt_one_minus_alpha_cum_prod*noise\n",
    "    \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)) / torch.sqrt(self.alpha_cum_prod[t]))\n",
    "        x0 = torch.clamp(x0, -1, 1)\n",
    "        \n",
    "        mean = xt - ((self.betas[t]*noise_pred)/(self.sqrt_one_minus_alpha_cum_prod[t]))\n",
    "        mean = mean/torch.sqrt(self.alphas[t])\n",
    "\n",
    "        if t==0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod[t-1])/(1. - self.alpha_cum_prod[t])\n",
    "            variance = variance*self.betas[t]\n",
    "            sigma = variance**0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "\n",
    "            return mean + sigma*z, x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, t_emb_dim):\n",
    "    factor = 1000**((torch.arange(start=0, end=t_emb_dim//2, device=time_steps.device)/(t_emb_dim//2)))\n",
    "\n",
    "    t_emb = time_steps[:, None].repeat(1, t_emb_dim//2)/factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = torch.nn.ModuleList(\n",
    "            [torch.nn.GroupNorm(8, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = torch.nn.ModuleList(\n",
    "            [torch.nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.residual_input_conv = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.down_sample_conv = torch.nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else torch.nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Attention block of Unet\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = torch.nn.ModuleList(\n",
    "            [torch.nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = torch.nn.ModuleList(\n",
    "            [torch.nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.residual_input_conv = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.GroupNorm(8, out_channels),\n",
    "                    torch.nn.SiLU(),\n",
    "                    torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_input_conv = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.up_sample_conv = torch.nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1) if self.up_sample else torch.nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3dbfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, im_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.down_channels = [32, 64, 128, 256]\n",
    "        self.mid_channels = [256, 256, 128]\n",
    "        self.t_emb_dim = 128\n",
    "        self.down_sample = [True, True, False]\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.t_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = torch.nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        self.downs = torch.nn.ModuleList()\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim, down_sample=self.down_sample[i], num_heads=4, num_layers=self.num_layers))\n",
    "        \n",
    "        self.mids = torch.nn.ModuleList()\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim, num_heads=4, num_layers=self.num_layers))\n",
    "\n",
    "        self.ups = torch.nn.ModuleList()\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpBlock(self.down_channels[i]*2, self.down_channels[i-1] if i!=0 else 16, self.t_emb_dim, up_sample=self.down_sample[i], num_heads=4, num_layers=self.num_layers))\n",
    "        \n",
    "        self.norm_out = torch.nn.GroupNorm(8, 16)\n",
    "        self.conv_out = torch.nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        out = self.conv_in(x)\n",
    "\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "\n",
    "        down_outs = list()\n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "\n",
    "        out = self.norm_out(out)\n",
    "        out = torch.nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "\n",
    "        return out        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=dataset_transforms, download=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=dataset_transforms, download=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "scheduler = LinearNoiseScheduler(num_timesteps=NUM_TIMESTEPS, beta_start=BETA_START, beta_end=BETA_END, device=device)\n",
    "\n",
    "model = UNet(im_channels=1)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "    losses = list()\n",
    "    for im, _ in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        im = im.to(device)\n",
    "        noise = torch.randn_like(im).to(device)\n",
    "        t = torch.randint(0, NUM_TIMESTEPS, (im.shape[0],)).to(device)\n",
    "\n",
    "        noisy_im = scheduler.add_noise(im, noise, t)\n",
    "\n",
    "        noise_pred = model(noisy_im, t)\n",
    "\n",
    "        loss = criterion(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch: {epoch_idx+1}, Train Loss: {np.mean(losses):.5f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModelSampler():\n",
    "    def __init__(self, T, model, beta_start, beta_end, device):\n",
    "        self.T = T\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = torch.linspace(beta_start, beta_end, T).to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "\n",
    "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
    "        self.sigmas = torch.sqrt(1.0 - self.alpha_bar)\n",
    "        self.lambdas = torch.log(self.sqrt_alpha_bar/self.sigmas)\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def simple_sampling(self, scheduler, n_samples=1, image_channels=1, img_size=(28, 28)):\n",
    "        xt = torch.randn((n_samples, image_channels, img_size[0], img_size[1]), device=self.device)\n",
    "\n",
    "        for t in reversed(range(self.T)):\n",
    "            noise_pred = self.model(xt, torch.as_tensor(t).unsqueeze(0).to(self.device))\n",
    "\n",
    "            xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))\n",
    "        \n",
    "        \n",
    "        img = torch.clamp(xt, -1, 1).detach().cpu()\n",
    "        img = (img+1)/2\n",
    "\n",
    "        return img\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), n_steps=50):\n",
    "        step_size = self.T//n_steps\n",
    "        xt = torch.randn((n_samples, image_channels, img_size[0], img_size[1]), device=self.device)\n",
    "\n",
    "        for tau in range(n_steps):\n",
    "            t = self.T - tau*step_size # compute the time t\n",
    "            t_tensor = torch.ones(n_samples, dtype=torch.long, device=self.device)*t\n",
    "\n",
    "            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            alpha_bar_prev = self.alpha_bar[t-step_size-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) if t>step_size else torch.tensor(1.0).to(self.device)\n",
    "\n",
    "            noise_pred = self.model(xt, t_tensor-1)\n",
    "            x0_pred = (xt - torch.sqrt(1 - alpha_bar_t)*noise_pred)/torch.sqrt(alpha_bar_t)\n",
    "            dir_xt = torch.sqrt(1 - alpha_bar_prev)*noise_pred\n",
    "\n",
    "            xt = torch.sqrt(alpha_bar_prev)*x0_pred + dir_xt\n",
    "        \n",
    "        img = torch.clamp(xt, -1, 1).detach().cpu()\n",
    "        img = (img+1)/2\n",
    "\n",
    "        return img\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def dpm_solver_sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), n_steps=10):\n",
    "        step_size = self.T//n_steps\n",
    "\n",
    "        xt = torch.randn((n_samples, image_channels, img_size[0], img_size[1]), device=self.device)\n",
    "        x_tilde = xt\n",
    "\n",
    "        for tau in range(n_steps):\n",
    "            t_prev = self.T - tau*step_size\n",
    "            t_cur = max(t_prev-step_size, 1)\n",
    "\n",
    "            lam_mid = (self.lambdas[t_prev - 1] + self.lambdas[t_cur - 1])/2.\n",
    "            s_i = torch.argmin(torch.abs(self.lambdas - lam_mid)).item() + 1\n",
    "\n",
    "            h = self.lambdas[t_cur - 1] - self.lambdas[t_prev - 1]\n",
    "\n",
    "            t_prev_tensor = torch.full((n_samples,), t_prev, dtype=torch.long, device=self.device)\n",
    "\n",
    "            u_i = (self.sqrt_alpha_bar[s_i - 1] / self.sqrt_alpha_bar[t_prev - 1])*x_tilde - self.sigmas[s_i - 1]*(torch.exp(h*0.5) - 1)*self.model(x_tilde, t_prev_tensor - 1)\n",
    "\n",
    "            t_s_tensor = torch.full((n_samples,), s_i, dtype=torch.long, device=self.device)\n",
    "\n",
    "            x_tilde = (self.sqrt_alpha_bar[t_cur - 1]/self.sqrt_alpha_bar[t_prev - 1]) * x_tilde - self.sigmas[t_cur - 1]*(torch.exp(h) - 1)*self.model(u_i, t_s_tensor - 1)\n",
    "        \n",
    "\n",
    "        img = torch.clamp(x_tilde, -1, 1).detach().cpu()\n",
    "        img = (img+1)/2\n",
    "        \n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pndm_sampling(self, n_samples=1, image_channels=1, img_size=(28, 28), n_steps=50):\n",
    "        step_size = self.T//n_steps\n",
    "        time_steps = [self.T - i*step_size for i in range(n_steps)]\n",
    "        \n",
    "        if time_steps[-1] != 0:\n",
    "            time_steps.append(0)\n",
    "\n",
    "        xt = torch.randn((n_samples, image_channels, img_size[0], img_size[1]), device=self.device)\n",
    "\n",
    "        eps_buffer = list()\n",
    "        self.counter = 0\n",
    "\n",
    "        for t, t_next in zip(time_steps[:-1], time_steps[1:]):\n",
    "            if self.counter<3:\n",
    "                xt, e_t = self._step_prx(xt, t, t_next)\n",
    "            else:\n",
    "                xt, e_t = self._step_plms(xt, t, t_next, eps_buffer)\n",
    "\n",
    "            eps_buffer.append(e_t)\n",
    "\n",
    "            if len(eps_buffer)>3:\n",
    "                eps_buffer.pop(0)\n",
    "\n",
    "            self.counter+=1\n",
    "\n",
    "\n",
    "        img = torch.clamp(xt, -1, 1).detach().cpu()\n",
    "        img = (img+1)/2\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _step_prx(self, xt, t, t_next):\n",
    "        delta = t - t_next\n",
    "        tm = int(t - delta/2)\n",
    "        \n",
    "        t_vec = torch.full((xt.shape[0],), t, dtype=torch.long, device=self.device)\n",
    "        tm_vec = torch.full((xt.shape[0],), tm, dtype=torch.long, device=self.device)\n",
    "        tnext_vec = torch.full((xt.shape[0],), t_next, dtype=torch.long, device=self.device)\n",
    "\n",
    "        e1 = self.model(xt, t_vec)\n",
    "        x1 = self._phi(xt, e1, t, tm)\n",
    "        e2 = self.model(x1, tm_vec)\n",
    "        x2 = self._phi(xt, e2, t, tm)\n",
    "        e3 = self.model(x2, tm_vec)\n",
    "        x3 = self._phi(xt, e3, t, t_next)\n",
    "        e4 = self.model(x3, tnext_vec)\n",
    "\n",
    "        e_prime = (e1 + 2*e2 + 2*e3 +e4)/6.0\n",
    "        x_next = self._phi(xt, e_prime, t, t_next)\n",
    "\n",
    "        return x_next, e_prime\n",
    "    \n",
    "    def _step_plms(self, xt, t, t_next, eps_buffer):\n",
    "        t_vec = torch.full((xt.shape[0],), t, dtype=torch.long, device=self.device)\n",
    "        e_t = self.model(xt, t_vec)\n",
    "\n",
    "        past = torch.stack([e_t, eps_buffer[-1], eps_buffer[-2], eps_buffer[-3]], dim=0)\n",
    "        e_prime = (55*past[0] - 59*past[1] + 37*past[2] - 9*past[3])/24.0\n",
    "\n",
    "        x_next = self._phi(xt, e_prime, t, t_next)\n",
    "\n",
    "        return x_next, e_t\n",
    "    \n",
    "    def _phi(self, xt, eps, t, t_next):\n",
    "        if t>0:\n",
    "            ab_t = self.alpha_bar[t-1]\n",
    "        else:\n",
    "            ab_t = torch.tensor(1.0, device=self.device)\n",
    "        \n",
    "        if t_next>0:\n",
    "            ab_next = self.alpha_bar[t_next - 1]\n",
    "        else:\n",
    "            ab_next = torch.tensor(1.0, device=self.device)\n",
    "        \n",
    "        denom = ab_t.sqrt() * (((1-ab_next).sqrt())*ab_t.sqrt() + ((1 - ab_t).sqrt())*ab_next.sqrt())\n",
    "\n",
    "        return (ab_next.sqrt()/ab_t.sqrt())*xt - ((ab_next - ab_t)/denom)*eps\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"DDPM_trained_SL.pt\")\n",
    "dm_sampler = DiffusionModelSampler(NUM_TIMESTEPS, model, BETA_START, BETA_END, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_times = list()\n",
    "for i in range(100):\n",
    "    tic = time.time()\n",
    "    gen_img = dm_sampler.simple_sampling(scheduler=scheduler)\n",
    "    toc = time.time()\n",
    "    generation_times.append(toc-tic)\n",
    "    plt.imshow(gen_img[0, 0], cmap=\"gray\")\n",
    "    plt.savefig(f\"gen_img/DDPM/img_{i}.png\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=10) for _ in range(10)]\n",
    "list_of_50_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=50) for _ in range(10)]\n",
    "\n",
    "print(f\"DDIM generation time (1 sample): {np.mean(generation_times):.2f}+-{np.std(generation_times):.4f}\")\n",
    "print(f\"DDIM generation time (10 samples): {np.mean(list_of_10_selected ):.2f}+-{np.std(list_of_10_selected):.4f}\")\n",
    "print(f\"DDIM generation time (50 samples): {np.mean(list_of_50_selected):.2f}+-{np.std(list_of_50_selected):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_times = list()\n",
    "for i in range(100):\n",
    "    tic = time.time()\n",
    "    gen_img = dm_sampler.ddim_sampling()\n",
    "    toc = time.time()\n",
    "    generation_times.append(toc-tic)\n",
    "    plt.imshow(gen_img[0, 0], cmap=\"gray\")\n",
    "    plt.savefig(f\"gen_img/DDIM/img_{i}.png\", dpi=600)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=10) for _ in range(10)]\n",
    "list_of_50_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=50) for _ in range(10)]\n",
    "\n",
    "print(f\"DDIM generation time (1 sample): {np.mean(generation_times):.2f}+-{np.std(generation_times):.4f}\")\n",
    "print(f\"DDIM generation time (10 samples): {np.mean(list_of_10_selected ):.2f}+-{np.std(list_of_10_selected):.4f}\")\n",
    "print(f\"DDIM generation time (50 samples): {np.mean(list_of_50_selected):.2f}+-{np.std(list_of_50_selected):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb68f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_times = list()\n",
    "for i in range(100):\n",
    "    tic = time.time()\n",
    "    gen_img = dm_sampler.dpm_solver_sampling()\n",
    "    toc = time.time()\n",
    "    generation_times.append(toc-tic)\n",
    "    plt.imshow(gen_img[0, 0], cmap=\"gray\")\n",
    "    plt.savefig(f\"gen_img/DPM/img_{i}.png\", dpi=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=10) for _ in range(10)]\n",
    "list_of_50_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=50) for _ in range(10)]\n",
    "\n",
    "print(f\"DDIM generation time (1 sample): {np.mean(generation_times):.2f}+-{np.std(generation_times):.4f}\")\n",
    "print(f\"DDIM generation time (10 samples): {np.mean(list_of_10_selected ):.2f}+-{np.std(list_of_10_selected):.4f}\")\n",
    "print(f\"DDIM generation time (50 samples): {np.mean(list_of_50_selected):.2f}+-{np.std(list_of_50_selected):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_times = list()\n",
    "for i in range(100):\n",
    "    tic = time.time()\n",
    "    gen_img = dm_sampler.pndm_sampling()\n",
    "    toc = time.time()\n",
    "    generation_times.append(toc-tic)\n",
    "    plt.imshow(gen_img[0, 0], cmap=\"gray\")\n",
    "    plt.savefig(f\"gen_img/PN/img_{i}.png\", dpi=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff173e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_10_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=10) for _ in range(10)]\n",
    "list_of_50_selected = [get_sum_of_random_numbers(generation_times, num_of_samples=50) for _ in range(10)]\n",
    "\n",
    "print(f\"DDIM generation time (1 sample): {np.mean(generation_times):.2f}+-{np.std(generation_times):.4f}\")\n",
    "print(f\"DDIM generation time (10 samples): {np.mean(list_of_10_selected ):.2f}+-{np.std(list_of_10_selected):.4f}\")\n",
    "print(f\"DDIM generation time (50 samples): {np.mean(list_of_50_selected):.2f}+-{np.std(list_of_50_selected):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
